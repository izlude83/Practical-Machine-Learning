---
title: "Course Project for Practical Machine Learning"
author: "Steven"
date: "21 December 2017"
output: html_document
---

### Summary

The aim of this project is to utilise data collected from personal activity devices and predict the manner they performed the exercise through development of a model. 


### Exploratory Data Analysis and Manipulation

We first load the data into R and check out what it contains. 

```{r}
set.seed(111)
training <- read.csv("pml-training.csv", header = TRUE)
testing <- read.csv("pml-testing.csv", header = TRUE)
```

The first 5 columns comprising X (running number), user name, and timestamps are not relevant given the aim of this project, and can be removed from the datasets. 

```{r}
training1 <- training[, c(6:160)]
testing1 <- testing[, c(6:160)]
```

The new_window represents a new time window for sliding window feature extraction. Given that it comprises mostly "no" values, it is not likely to have much bearing on the model and its resultant predictions, and can be removed as well. 

```{r}
summary(training$new_window)
training2 <- training1[, c(2:155)]
testing2 <- testing1[, c(2:155)]
```

There are numerous variables which denote the max, min, var, stddev, avg, kurtosis, skewness and amplitude of the raw measurements. Since these are derived variables based on raw measurements which are already represented in the dataset and contain mostly blank or NA values, they are not likely to be of much use in this model and can be removed as well. 

```{r}
columnstoremove1 <- c(grep("max_", names(training2)), grep("min_", names(training2)), grep("var_", names(training2)), grep("stddev_", names(training2)), grep("avg_", names(training2)), grep("kurtosis", names(training2)), grep("skewness", names(training2)), grep("amplitude_", names(training2)))
training3 <- training2[, -columnstoremove1]
testing3 <- testing2[, -columnstoremove1]
dim(training3)
```

We are now left with 54 variables comprising mainly raw measurements.


### Model Development and Cross Validation

We will use the train() function under caret with method randomForest to develop the required model. This is because the caret package automates the process of fitting multiple versions of a given model by varying its folds within a cross-validation process, and is able to estimate an out of sample error by aggregating the accuracy analysis across training runs. 

The train() function coupled with the randomForest method and cross-validation would take a long processing time, and we will use the parallel package to provide parallel processing capabilities which would give a more manageable response time. 

```{r}
library(parallel)
library(doParallel)
library(caret)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

```

Next, we set the parameters to allow train to use cross-validtaion, and define the number of folds for k-fold cross-validation. We will use k = 3 for this project. A small k value used leads to more bias, but less variance. 

```{r}
m1ctrl <- trainControl(method = "cv", number = 3, allowParallel = TRUE)

```

We then proceed to run the model and the required cross-validation. 

```{r}
m1 <- train(classe ~. , data = training3, method = "rf", trControl = m1ctrl)
stopCluster(cluster)
registerDoSEQ()

```

Next, we check out the generated model to determine the accuracy it can provide. 

```{r}
m1
confusionMatrix.train(m1)
```

The resultant model generated using 3-fold cross-validation has more than 99% accuracy which should suffice for our purpose. 

### Prediction

Next, we will use the model to predict the manner in which the activity was carried out for each of the 20 test cases provided under the test file. 

```{r}
predict(m1, testing3)
```

The above results were entered into Coursera's Week 4 online quiz with 20/20 accuracy, which shows that the model we have derived works reasonably well for this purpose. 